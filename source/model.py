from typing import Listimport kerasimport tensorflow as tffrom source import utilsfrom tensorflow import set_random_seedfrom keras.initializers import npfrom keras.backend.tensorflow_backend import expand_dims, squeezefrom keras.layers import Input, Lambda, LSTM, CuDNNLSTM, Bidirectional, Dense, ReLU, TimeDistributed, \                         BatchNormalization, Dropout, ZeroPadding2D, Conv2D, Reshapeclass Model(keras.Model):               # This class is required for a consistent typing    is_adversarial: bool = False    adversarial_weight: float = Nonedef deepspeech(is_gpu: bool, input_dim=80, output_dim=36, context=7,               units=1024, dropouts=(0.1, 0.1, 0), random_state=1) -> Model:    """    Model is adapted from: Deep Speech: Scaling up end-to-end speech recognition (https://arxiv.org/abs/1412.5567)    It is worth to know which default parameters are used:    - Conv2D:        strides: (1, 1)        padding: "valid"        dilatation_rate: 1        activation: "linear"        use_bias: True        data_format: "channels last"        kernel_initializer: "glorot_uniform"        bias_initializer: "zeros"    - Dense:        activation: "linear"        use_bias: True        kernel_initializer: "glorot_uniform"        bias_initializer: "zeros"    - LSTM (as for CuDNNLSTM):        use_bias: True,        kernel_initializer: "glorot_uniform"        recurrent_initializer: "orthogonal"        bias_initializer: "zeros"        unit_forget_bias: True        implementation: 1        return_state: False        go_backwards: False        stateful: False        unroll: False    """    np.random.seed(random_state)    set_random_seed(random_state)                                                   # Create model under CPU scope and avoid OOM    with tf.device('/cpu:0'):                                                       # errors during concatenation a large distributed model.        input_tensor = Input([None, input_dim], name='X')                           # Define input tensor [time, features]        tensor = Lambda(expand_dims, arguments=dict(axis=-1))(input_tensor)         # Add 4th dim (channel)        tensor = ZeroPadding2D(padding=(context, 0))(tensor)                        # Fill zeros around time dimension        receptive_field = (2*context + 1, input_dim)                                # Take into account fore/back-ward context        tensor = Conv2D(filters=units, kernel_size=receptive_field)(tensor)         # Convolve signal in time dim        tensor = Lambda(squeeze, arguments=dict(axis=2))(tensor)                    # Squeeze into 3rd dim array        tensor = ReLU(max_value=20)(tensor)                                         # Add non-linearity        tensor = Dropout(rate=dropouts[0])(tensor)                                  # Use dropout as regularization        tensor = TimeDistributed(Dense(units))(tensor)                              # 2nd and 3rd FC layers do a feature        tensor = ReLU(max_value=20)(tensor)                                         # extraction base on the context        tensor = Dropout(rate=dropouts[1])(tensor)        tensor = TimeDistributed(Dense(units))(tensor)        tensor = ReLU(max_value=20)(tensor)        tensor = Dropout(rate=dropouts[2])(tensor)        tensor = Bidirectional(CuDNNLSTM(units, return_sequences=True)              # LSTM handle long dependencies                               if is_gpu else LSTM(units, return_sequences=True),                               merge_mode='sum')(tensor)        tensor = TimeDistributed(Dense(output_dim, activation='softmax'))(tensor)   # Return at each time step prob along characters        output_tensor = utils.rename([tensor], ['main_output'])        model = Model(input_tensor, output_tensor, name='DeepSpeech')    return modeldef deepspeech_custom(is_gpu: bool, layers: List[dict], input_dim: int, is_adversarial: bool = False,                      adversarial_weight: float = 1, to_freeze: List[str] = None, random_state=1) -> Model:    np.random.seed(random_state)    set_random_seed(random_state)    constructors = {        'BatchNormalization': lambda params: BatchNormalization(**params),        'Conv2D': lambda params: Conv2D(**params, name=name),        'Dense': lambda params: Dense(**params, name=name),        'DenseTimeDistributed': lambda params: TimeDistributed(Dense(**params), name=name),        'Dropout': lambda params: Dropout(**params),        'LSTM': lambda params: Bidirectional(CuDNNLSTM(**params) if is_gpu else                                             LSTM(activation='tanh', recurrent_activation='sigmoid', **params),                                             merge_mode='sum', name=name),        'ReLU': lambda params: ReLU(**params),        'ZeroPadding2D': lambda params: ZeroPadding2D(**params),        'expand_dims': lambda params: Lambda(expand_dims, arguments=params),        'squeeze': lambda params: Lambda(squeeze, arguments=params),        'squeeze_last_dims': lambda params: Reshape([-1, params['units']]),        'rename': lambda name: Lambda(lambda x: x, name=name)    }    with tf.device('/cpu:0'):        input_tensor = Input([None, input_dim], name='X')        tensor = input_tensor        aux_tensor = None                                               # This is placeholder for the auxiliary gradient path.        for params in layers:            name = params.pop('name', '')                               # `name` is implicit passed to constructors            constructor_name = params.pop('constructor')            is_adversarial_layer = params.pop('is_adversarial_layer', False)            constructor = constructors[constructor_name]                # Conv2D, TimeDistributed and Bidirectional.            layer = constructor(params)            if is_adversarial_layer:                aux_tensor = layer(tensor)                              # The Adversarial Network has two gradient paths.                continue            tensor = layer(tensor)        tensors = [tensor, aux_tensor] if aux_tensor is not None else [tensor]      # Rename target tensors to have easy access to them.        output_tensors = utils.rename(tensors, names=['main_output', 'is_synthesized'])        model = Model(input_tensor, output_tensors, name='DeepSpeech')        model.is_adversarial = is_adversarial        model.adversarial_weight = adversarial_weight        if to_freeze:            utils.freeze_layers(model, names=to_freeze)    return model